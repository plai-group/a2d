\label{sec:discussion}

In this work we have discussed learning policies in POMDPs.  Partial information and high-dimensional observations can make direct application of RL expensive and unreliable.  Asymmetric learning uses additional information to improve performance beyond comparable symmetric methods.  Asymmetric IL can efficiently learn a partially observing policy by imitating an omniscient expert.  However, this approach requires a pre-existing expert, and, critically, assumes that the expert can provide suitable supervision -- a condition we formalize as identifiability.  The learned trainee can perform arbitrarily poorly when this is not satisfied.  We therefore develop adaptive asymmetric DAgger (A2D), which adapts the expert policy such that AIL can efficiently recover the optimal partially observed policy.  A2D also allows the expert to be learned online with the agent, and hence does not require any pretrained artifacts.  

There are three notable extensions of A2D.  The first extension is investigating more conservative updates for the expert and trainee which take into consideration the limitations or approximate nature of each intermediate update.  The second extension is studying the behavior of A2D in environments where the expert is not omniscient, but observes a superset of the environment relative to the agent. The final extension is integrating A2D into differentiable planning methods, exploiting the low dimensional state vector to learn a latent dynamics model, or, improve sample efficiency in sparse reward environments.

We conclude by outlining under what conditions the methods discussed in this paper may be most applicable.  If a pretrained expert or example trajectories are available, AIL provides an efficient methodology that should be investigated first, but, that may fail catastrophically.  If the observed dimension is small, and no reliable expert is available, direct application of RL is likely to perform well.  If the observed dimension is large, and trajectories which adequately cover the state-space are available, then pretraining an image encoder can provide a competitive and flexible approach.  Finally, if a compact state representation is available alongside a high dimensional observation space, A2D offers an alternative that is robust and expedites training in high-dimensional and asymmetric environments.  
