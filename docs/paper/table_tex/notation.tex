
% Please add the following required packages to your document preamble:
\renewcommand{\arraystretch}{1.5}
\begin{minipage}{\textwidth}
\centering
% \begin{figure}[t]
\begin{adjustwidth}{-.5in}{-.5in}  
\begin{center}
\tiny
\begin{tabular}{@{}p{2cm}p{2cm}p{2cm}p{3cm}p{6.5cm}@{}}
\toprule
Symbol                             & Name                         & Alternative Name(s)                                             & Type                                                                                                                              & Description                                                                                                                                                                   \\ \hline
$t$                                & Time                         & Discrete time step                                              & $\mathbb{Z}$                                                                                                                      & Discrete time step used in integration.  Indexes other values.                                                                                                                \\
$s_t$                              & State                        & Full state, compact state, omniscient state                     & $\mathcal{S} = \mathbb{R}^D$                                                                                                      & State space of the MDP.  Sufficient to fully define state of the environment.                                                                                                 \\
$o_t$                              & Observation                  & Partial observation                                             & $\mathcal{O} = \mathbb{R}^{A\times B \times \dots}$                                                                               & Observed value in POMDP, emitted conditional on state.  State is generally not identifiable from observation. Conditionally dependent only on state.                          \\
$a_t$                              & Action                       &                                                                 & $\mathcal{A} = \mathbb{R}^K$                                                                                                      & Interaction made with the environment at time $t$.                                                                                                                            \\
$r_t$                              & Reward                       &                                                                 & $\mathcal{R}$                                                                                                                     & Value received at time $t$ indicating performance.  Maximising sum of rewards is the objective.                                                                               \\
$b_t$                              & Belief state                 &                                                                 & $\mathcal{B}$                                                                                                                     &                                                                                                                                                                               \\
$q_{\pi}$                          & Trajectory distribution      &                                                                 & $\mathcal{Q} : \Pi \rightarrow (\mathcal{A} \times \mathcal{B} \times \mathcal{O} \times \mathcal{S}^2 \times \mathcal{R})^{t+1}$ & Process of sampling trajectories using the policy $\pi$.  If the process is fully observed $\mathcal{O} = \emptyset$.                                                         \\
$\tau_{0:t}$                       & Trajectory                   & Rollouts                                                        & $(\mathcal{A} \times \mathcal{B} \times \mathcal{O} \times \mathcal{S}^2 \times \mathcal{R})^{t+1}$                               & Sequence of tuples containing state, next state, observation, action and reward.                                                                                              \\
$\gamma$                           & Discount factor              &                                                                 & $\Gamma = \left[  0, 1 \right]$                                                                                                   & Factor attenuating future reward in favor of near reward.                                                                                                                    \\
$p(s_{t+1} | s_t, a_t)$            & Transition distribution      & Plant model, environment                                        & $\mathcal{T} : \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$                                                            & Defines how state evolves, conditional on the previous state and the action taken.                                                                                            \\
$p(o_t | s_t)$                     & Emission distribution        & Observation function                                            & $\mathcal{Y} : \mathcal{S} \rightarrow \mathcal{O}$                                                                               & Distribution over observed values conditioned on state.                                                                                                                       \\
$p(s_0)$                           & Initial state distribution   & State prior                                                     & $\mathcal{T}_0 : \rightarrow \mathcal{S}$                                                                                         & Distribution over state at $t=0$.                                                                                                                                             \\
$\pi_{\theta}(a_t | s_t)$          & MDP policy                   & Expert, omniscient policy, asymmetric expert, asymmetric policy & $\Pi_{\Theta} : \mathcal{S} \rightarrow \mathcal{A}$                                                                              & Distribution over actions conditioned on state.  Only used in MDP.                                                                                                            \\
$\theta$                           & MDP policy parameters        &                                                                 & $\Theta$                                                                                                                          & Parameters of MDP policy.  Cumulative reward is maximized over these parameters.                                                                                              \\
$\pi_{\phi}(a_t | b_t)$            & POMDP policy                 & Agent, partially observing policy                               & $\Pi_{\Phi} : \mathcal{B} \rightarrow \mathcal{A}$                                                                                & Distribution over actions conditioned on belief state.  Only used in POMDP.                                                                                                   \\
$\phi$                             & POMDP policy parameters      &                                                                 & $\Phi$                                                                                                                            & Parameters of MDP policy.  Cumulative reward is maximized over these parameters.                                                                                              \\
$\pi_{\psi}(a_t | b_t)$            & Variational trainee policy                 & Variational approximation                               & $\Pi_{\Psi} : \mathcal{B} \rightarrow \mathcal{A}$                                                                                & Variational approximation of the implicit policy.                                                                                                   \\
$\psi$                             & Variational trainee policy parameters      &                                                                 & $\Psi$                                                                                                                            & Parameters of the variational approximation of the implicit policy.                                                                                              \\
$\pi_{\beta}$                      & Mixture policy               &                                                                 & $\Pi_{\beta} : \mathcal{S} \times \mathcal{B} \rightarrow \mathcal{A}$                                                            & Mixture of MDP policy ($\pi_{\theta}$) and POMDP policy ($\pi_{\phi}$).                                                                                                       \\
$\beta$                            & Mixing coefficient           &                                                                 & $\left[ 0, 1 \right]$                                                                                                             & Fraction of MDP policy used in mixture policy.                                                                                                                                \\
$D$                                & Replay buffer                & Data buffer                                                     & $\mathcal{D} = \left\lbrace \tau_{0:T_n} \right\rbrace_{n \in 1:N}$                                                               & Store to access previous trajectories.  Facilitates data re-use.                                                                                                              \\
$\mathbb{KL}\left[ p || q \right]$ & Kullbackâ€“Leibler divergence  & KL divergence, forward KL, mass-covering KL                     &                                                                                                                                   & Particular divergence between two distributions.  Forward KL is mass covering.  Reverse KL ($\mathbb{KL} \left[ q || p \right]$) is mode seeking.                             \\
$Q^{\pi}(s_t, a_t)$                & Q-function                   & State Q-function                                                & $\mathcal{Q}_s : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$                                                           & Expected sum of rewards ahead, garnered by taking action $a_t$ in state $s_t$ induced by policy $\pi$.                                                                        \\
$Q^{\pi}(b_t, a_t)$                & Belief state Q-function      &                                                                 & $\mathcal{Q}_b : \mathcal{B} \times \mathcal{A} \rightarrow \mathbb{R}$                                                           & Expected sum of rewards ahead, garnered by taking action $a_t$ in belief state $b_t$ induced by policy $\pi$.                                                                 \\
$\hat{\pi}_{\theta}(a_t | b_t)$    & Implicit policy &                                                                 & $\Pi_{\Phi} : \mathcal{B} \rightarrow \mathcal{A}$                                                                                & Agent policy obtained by marginalizing over state given belief state.   Closest approximation of $\pi_{\theta}$ under partial observability.  Approximated by $\pi_{\phi}$.   \\
% $\hat{Q}^{\pi_{\phi}}(s_t, a_t)$   & Belief-marginal Q-function   &                                                                 & $\mathcal{Q}_s : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$                                                           & Q-function, conditioned on state, obtained by rolling out under belief-state conditioned policy $\pi_{\phi}$.                                                                 \\
$d^{\pi}(s_t, b_t)$                & Occupancy                    & Discounted state visitation distribution~\citep{pmlr-v125-agarwal20a}                                                                & $M : \mathcal{S} \times \mathcal{B} \rightarrow \mathbb{R}$                                                                     & Joint density of $s_t = s$ and $b_t = b$ given policy $\pi$. Marginal of $q_{\pi}$ over previous and future states, belief states, and all actions, observations and rewards. \\
$\pi_{\eta}$                & Fixed reference distribution                    &                                                                 & $\Pi$                                                                     & Fixed distribution that is rolled out under to generate samples that are used in gradient calculation. \\\hline
\end{tabular}
\end{center}
\end{adjustwidth}
\captionof{table}{Notation and definitions used throughout the main paper.}
\label{tab:notation}
% \end{figure}
\end{minipage}