We now present a comprehensive review of existing literature not already covered.  Exploiting asymmetric learning to accelerate learning has been explored in numerous previous work under a number of different frameworks, application domains, and levels of theoretical analysis.

The notion of using fully observed states unavailable at deployment time is often referred to as exploiting ``privileged information''~\citep{vapnik2009new, lambert2018deep}.  For clarity, we refer to the expert as having access to privileged information, and the agent as only having access to a partial observation.  We note that the use of the term ``expert'' does not imply that this policy is necessarily optimal under the MDP.  Indeed, in A2D, the expert is co-trained with the agent, such that the expert is approximately a uniform random distribution at the start of the learning procedure.  The term privileged information is more general than simply providing the world state, and may include additional loss terms or non-trivial transforms of the world state that expedite learning the agent.  In this work, we exclusively consider the most general scenario where the privileged information is the full world state.  However, there is nothing precluding defining an extended state space to include hand-designed features extracted from the state, or, using additional, hand crafted reward shaping terms when learning (or adapting) the expert. 

\subsection{Encodings}
The first use-case we examine is probably the simplest, and the most widely studied.  Asymmetric information is used to learn an encoding of the observation that reduces the dimensionality while retaining information.  Standard reinforcement learning approaches are then employed freezing this encoding.  Two slight variations on this theme exist.  In the first approach, an MDP policy is learned to generate rollouts conditioned on omniscient information, and an encoder is learned on state-observation pairs visited during these rollouts~\citep{Finn2016, Levine2016}.  Either the encoder acts to directly recover the underlying states, or simply learns a lower-dimensional embedding where performing reinforcement learning is more straightforward.

\citet{Andrychowicz2020} explore learning to manipulate objects using a mechanical hand using \emph{both} state information from the robot (joint poses, fingertip positions etc) and RGB images.  This particular application is an interesting hybrid approach dictated by the domain.  State information pertaining to the manipulator is easily obtained, but state information about the pose of the object being manipulated is unavailable and must be recovered using the images.  A controller is learned in simulation (MDP), while simultaneously (and separately from the MDP) a separate perception network is learned that maps the image to the pose of the object being manipulated.  State information and pose encoding are then concatenated and used as the state vector on which the policy acts.  While the pose of the object is unobserved, it is readily recoverable from a single frame (or stack of frames), and hence the partial observation is predominantly a high-dimensional and bijective embedding of the true state.  If the true position of the hand was not available, this would be less certain as the object and other parts of the manipulator obfuscates much of the manipulator from any of the three viewpoints (more viewpoints would of course reverse this to being a bijection).  The use of a recurrent policy further improves the recovery of state as only the innovation in state needs to be recovered. 

\subsection{Asymmetric values}
Another well-explored use-case is to instead exploit asymmetric information for to improve learning a value or Q- function~\citep{kononen2004asymmetric, pinto2017asymmetric, Andrychowicz2020}.  This is achieved by conditioning either the value function or Q-function on different information than the policy that is either more informative, or lower dimensional representations, and can help guide learning~\cite{kononen2004asymmetric, pinto2017asymmetric}.  Learning the value or Q function in a lower-dimensional setting enables this function to be learned more stably and with fewer samples, and hence can track the current policy more effectively.  Since the value and Q-function are not used at test time, there is no requirement for privileged information to be available when deployed.  \citet{pinto2017asymmetric} introduce this in a robotics context, using an asymmetric value function, conditioned on the true underlying state of a robotic manipulator, to learn a partially observing agent conditioned only on a third-person monocular view of the arm.  Similar ideas were explored previously by \citet{kononen2004asymmetric} in relation to semi-centralized multi-agent systems, where each agent only partially observes the world state, but a central controller is able to observe the whole state.  The state used by the central controller is used to evaluate the value of a particular world state, whilest each agent only acts on partial information.

\subsection{Behavioral Cloning \& Imitation Learning}
Behavioral cloning and imitation learning~\citep{pmlr-v80-kang18a, Ross2011}, introduced in Main Section 2.3, is, in our opinion, an under-explored avenue for expediting learning in noisy and high-dimensional partially observed processes.  The main observation is that this process separates learning to act and learning to perceive~\citep{Chen2019}.  The fully observing expert learns to act, without the presence of extraneous patterns or noise.  The agent then learns to perceive such that it can replicate the actions of the expert.  A major benefit of cloning approaches is that perception is reduced to a supervised learning task, with lower variance than the underlying RL task.  

\citet{pinto2017asymmetric} briefly assess using asymmetric DAgger as a baseline.  It is observed that the agent learns quickly, but actually converges to a worse solution than the asymmetric actor-critic solution.  This difference is attributed to the experts access to (zero variance) state information otherwise unavailable to the partially observing agent.  Our work builds on this observation, seeking to mitigate such weaknesses.  Surprisingly, and to the best of our knowledge, no work (including \citet{pinto2017asymmetric}) has provided and in-depth analysis of this method, or directly built off this idea.  

\citet{Chen2019} showed that large performance gains can be found in an autonomous vehicles scenario by using IL through the use of an asymmetric expert, specifically for learning to drive in the autonomous vehicle simulator CARLA~\citep{Dosovitskiy17}.  \citet{Chen2019} train an expert from trajectories, created by human drivers, using behavioral cloning conditioned on an encoded aerial rendering of the environment including privileged information unavailable to the agent at deployment time.  The aerial rendering facilitates extensive data augmentation schemes that would otherwise be difficult, or impossible, to implement in a symmetric setting.  The agent is then learned using DAgger-based imitation learning.  However, this general approach implicitly makes assumptions about the performance of the expert, as well as the underlying identifiability (as we define in Section \ref{sec:il-failure}) between the underlying fully and partially observed Markov decision processes.

Other works combine RL and IL to gain performance beyond that of the expert by considering that the expert is sub-optimal~\citep{choudhury2018data, Sun2018, weihs2020bridging}, where the performance differential is either as a result of asymmetry, or, the expert simply not being optimal.  These works, most often, train a policy that exploits knowledge of the performance differential between the expert and agent, or, the difference in policies.  The weight applied to the sample in IL is increased for policies that are similar, or, where the performance gap is small.  The example is then down-weighted when it is believed that the expert provides poor supervision in that state.  However, these works do not consider updating the expert, and instead focus on ameliorating the drawbacks of AIL using derived statistics.  In our work, we seek to define a method for updating an expert directly. 


\subsection{Co-learning Expert and Agent}
Work that is maybe thematically most similar to ours investigates co-training of the agent and expert.  This builds on the AIL approach, where instead of assuming an optimal expert exists, the expert and agent policies are learned simultaneously, where either an additional training phase as added to ``align'' the expert and agent~\citep{salter2019attentionprivileged, Song2019}, architectural modification~\citep{pierrealex2020privileged}, or both~\citep{Schwab2019}.  An alternative method for deriving such an aligning gradient is to introduce an auxiliary loss regularizing the representation used by the agent to be predictive of the the underlying state, or, a best-possible belief representation~\citep{nguyen2020belief}.

\citet{salter2019attentionprivileged} trains separate policies for agent and expert using spatial attention, where the expert is conditioned on the state of the system, and the agent is conditioned on a monocular viewpoint.  By inspecting the attention map of expert and agent, it is simple to establish what parts of the state or image the policy is using to act.  An auxiliary (negative) reward term is added to the reward function that penalizes differences in the attention maps, such that the agent and expert are regularized to use the same underlying features.  This auxiliary loss term transfers information from the MDP to the POMDP.  The main drawbacks of this approach however are its inherent reliance on an attention mechanism, and tuning the hyperparameters dictating the weight of having a performant agent, expert and the level of alignment between the attention mechanisms.  Further, using a attention as the transfer mechanism between the agent and expert somewhat introduces an additional layer of complexity and obfuscation of the actual underlying mechanism of information transfer.

\citet{Song2019} present an algorithm, CoPiEr, that co-trains two policies, conditioned on different information (any combination of fully or partially observing).  CoPiEr rolls out under both policies separately, and then selects the rollouts from the policy that performs the best.  These samples are then used in either an RL or IL (or hybrid of the two) style update.  In this sense, the better performing policy (with ostensibly ``better'' rollouts) provides high-quality supervision to the policy with lower quality rollouts.  MDP to POMDP transfer or privileged information is not considered.  Most significantly, imitation learning is proposed as a method of transferring from one policy to another, or, RL augmented with an IL loss to provide better supervision while retaining RLs capability to explore policy space. 

\citet{Schwab2019} on the other hand extend \citet{pinto2017asymmetric} by introducing multitask reinforcement learning themes.  A ``task'' is uniquely described by the set of variables that the policy is conditioned on, such as images from different view points, true state information and proprioceptive information.  An input-specific encoder encodes each observation before mixing the encoded input features and passing these to a head network which outputs the actions.  Instead of aligning attention mechanisms (as per \citet{salter2019attentionprivileged}), \citet{Schwab2019} the head network is shared between tasks providing alignment between the single-input policies.  At test time, only those observations that are available need to be supplied to the policy, respecting the partial observability requirement at test time.  This approach does not explicitly use an expert, instead using a greater range of more informative information channels to efficiently learn the policy head, while simultaneously co-training the channel-specific encoders.

Finally, the work of \citet{pierrealex2020privileged} present privileged information dropout (PI-D).  The general approach of information dropout~\citep{Achille_2018} is to lean a model while randomly perturbing the internal state of the model, effectively destroying some information.  The hypothesis is that this forces the model to learn more robust and redundant features that can survive this corruption.  \citet{pierrealex2020privileged} use this theme by embedding both partial observation and state, where the state embedding is then used to corrupt (through multiplicative dropout) the internal state of the agent.  The PI expert is then able to mask uninformative patterns in the observations (using the auxiliary state information), facilitating more efficient learning.  The PI can then be easily marginalized out by not applying the dropout term.  Importantly however, reinforcement learning is still performed in the partially observing agent, a characteristic we wish to avoid due to the high-variance nature of this learning. 
