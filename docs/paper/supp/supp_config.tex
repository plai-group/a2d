\subsection{Gridworld}

We implemented both gridworld environments by adapting the \emph{MiniGrid} environment provided by \citet{gym_minigrid}.  For both gridworld experiments, the image is rendered as a $42 \times 42$ RGB image.  The agent has four actions available, moving in each of the compass directions.  Each movement incurs a reward of $-2$, hitting the weak patch of ice or tiger incurs a reward of $-100$, and reaching the goal incurs a reward of $20$.  Pushing the button in Tiger Door is free, but effectively costs $4$ due to its position, or $2$ in the Q function experiments.  Policy performance is evaluated every five steps by sampling $2000$ interactions under the stochastic policy.  A discount factor of $\gamma=0.995$ was used and an upper limit of $T=200$ is placed on the time horizon.

For experts and agents/trainees that use the compact representation, the policy is a two layer MLP that accepts a compact vector as input, with $64$ hidden units in each layer, outputting the log-probabilities of each action.  The value function uses the same architecture and input, but outputs a number representing the reward ahead.   The value function is learned by minimizing the mean squared error in the discounted reward-to-go.  $32$ batches are constructed from the rollout and are used to update the value function using ADAM~\citep{kingma2014adam} with a learning rate of $7 \times 10^{-4}$, for $25$ epochs.  Q functions are only used here with compact representations, and so we can simply append a one-hot encoding of the action to the (flat) input vector.  The Q function is then learned in the same way as the value function, except for with a slightly lower learning rate of $3\times 10^{-4}$.  Policies and value functions conditioned on images use a two layer convolutional encoder, each with $32$ filters, and a single output layer mapping to a flat hidden state with $50$ hidden units.  Image-based policies and value functions learn separate image encoders in the gridworld examples, whereas in the CARLA examples, a shared encoder is used.  This output is then used as input into a two layer MLP, each with $64$ hidden units, outputting the log-probabilities of each action or the expected discounted reward ahead.   L2 regularization is applied to all networks, with a coefficient of $0.001$.  

We use TRPO with batch sizes of $2,000$ and a trust region of $0.01$.  An entropy regularizer is applied directly to the advantages computed, with coefficient $1$.  We set $\lambda=0.95$ in the GAE calculation~\citep{schulman2015high}.  This trust region, regularization and $\lambda$ value are used throughout, unless otherwise stated.  Reinforcement learning in the POMDP (\emph{RL}) uses separate policies and value functions conditioned on the most recent image.  In asymmetric reinforcement learning (\emph{RL (Asym)}) the policy is conditioned on the image, but the value function takes the compact and omniscient state representation ($s_t$) as input.  Policies and value functions are then learned using the same process as before.

The policy learned by \emph{RL (MDP)} is then used as the expert in AIL (\emph{AIL}), where $2,000$ samples are collected at each iteration and appended to a rolling buffer of $5,000$ samples.  The KL-divergence between the expert and trainee action distributions is minimized by performing stochastic gradient descent, using ADAM with a learning rate of $3\times 10^{-4}$, using a batch size of $64$ for two whole epochs per iteration.  We find that the MDP converges within approximately $80,000$ environment interactions, and so we begin the AIL line at this value.  $\beta$ is annealed to zero after the first time step (as recommended by \citet{Ross2011}).  

For experiments using a pretrained encoder (\emph{Pre-Enc}), we roll out for $10,000$ environment interactions under a trained MDP expert from \emph{RL (MDP)} to generate the data buffer.  The encoder, that takes images as input and targets the true state vector, is learned by regressing the predictions on to the true state.  We perform $100$ training epochs with a learning rate of $3 \times 10^{-4}$.  We start this curve at the $80,000$ interactions required to train the expert from which the encoder is learned.  We use an asymmetric value function conditioned on the true state.  The encoder is then frozen and a two-layer, $64$ hidden unit MLP policy head is learned using TRPO.  We found that a lower trust region size of $0.005$ was required for Tiger Door to stably converge.  We confirmed separately for both pretrained encoders and AIL that the encoder class can represent and learn the required policies and transforms, and both converge to the solution of the MDP when conditioned on \emph{omniscient} image-based input.  

For A2D, expert and trainee policies are initialized from scratch, and are learned using the broadly the same settings as \emph{RL (MDP)} and \emph{AIL}.  In A2D, we decay $\beta$ with coefficient $0.8$ at each iteration, although faster $\beta$ decays did not hurt performance.  Slower $\beta$ decays can lead to higher and longer divergences during training, and can lead to the agent becoming trapped in local optima.  We use a higher entropy regularization coefficient, equal to $10$, finding that this increased regularization helped A2D avoid falling into local minima, although this can be further ameliorated by setting $\beta = 0$ throughout, as we do in the CARLA experiments.  We found for Frozen Lake that a lower $\lambda = 0.9$ value of yielded more stable convergence and a lower final policy divergence (we refer the reader to Section \ref{supp:sub:sub:q} for more information).  Value and Q functions are learned by individually targeting the sum of rewards ahead (i.e. is not back-propagated through any mixture).  We note that choosing to parameterize the mixture value function as the weighted sum of individual value functions is an assumption.  However, we note that we require $\beta \tends 0$ for the gradient to be unbiased.  In this limit the mixture is equal to just the value function of the agent.  Therefore, explicitly parameterizing the value function in this way \emph{ensures} that state information is removed from the estimation.  Exploring different ways of parameterizing the value function is a potential topic for future research.

In Section \ref{supp:sub:sub:q} we use a $\lambda$ value of $0.5$ in GAE~\citep{schulman2015high} (when not sweeping over $\lambda$).  We used an entropy regularizer of $0.02$ is applied directly to the surrogate loss.  We also use TRPO with a trust region KL-divergence of $0.001$. 


\subsection{CARLA Experiments}
We implemented our autonomous vehicle experiment using CARLA~\citep{Dosovitskiy17}. This scenario represents a car driving forward at the speed limit, while avoiding a pedestrian which may run out from behind a vehicle $50\%$ of the time, at a variable speed.  There are a total of $10$ waypoints, indicating the path the vehicle should take as produced by an external path planner.  We enforce that the scenario will end prematurely if one of the following occurs: a time limit of $90$ time-steps is reached, a collision with a static object, a lane invasion occurs, if a waypoint is not gathered within $35$ time-steps, or, the car's path is not within a certain distance of the nearest waypoint.  We found that inclusion of these premature endings was crucial for efficient learning.  The reward surface for this problem is generated using a PID controller which is computed using an example nominal trajectory.  The reward at any given time-step is defined as the product of the absolute difference between the agents actions and the optimal actions by a low-level PID controller to guide the vehicle to the next waypoint, and is bounded to lie in $[0,1]$.  

For the expert policy used both in AIL and A2D, we use a two layer MLP with $64$ hidden units and ReLU activations. The agent and trainee policies use a shared image encoder~\citep{laskin2020reinforcement,laskin_srinivas2020curl,yarats2021image}, followed by the same MLP architecture as the expert policy to generate actions.  The RL algorithm used in both the expert and agent RL updates is PPO~\citep{schulman2017proximal} with generalized advantage estimation (GAE)~\citep{schulman2015high}.  We detach the encoder during the policy update and learn the encoder during the value function update~\citep{laskin2020reinforcement,laskin_srinivas2020curl,yarats2021image}. In A2D we use the MLP defined above for the expert policy.  The trainee policy and value functions use a common encoder, updated during the trainees value update and frozen during the policy update, and the MLP defined above as the policy head or value head network.  For all algorithms we used a batch size of $64$ in both the PPO policy update, value function update, and the imitation learning update.  As in the previous experiments, in the imitation learning step, we iterate through all data seen and stored in the replay buffer. We found that starting the $\beta$ parameter at zero produced faster convergence.

We performed a coarse-grained hyperparameter search using the Bayesian optimization routine provided by the experimental control and logging software \emph{Weights \& Biases}~\citep{wandb}.  This allows us to automate hyperparameter search and distribute experimental results for more complex experiments in a reproducible manner.  Each method was provided approximately the same amount of search time, evaluating at least $60$ different hyperparameter settings.  The optimal settings were then improved manually over the course of approximately $5$ further tests.  We score each method and hyperparameter setting using a running average of the reward over the previous $25$ evaluation steps, and used early stopping if a run consistently performed poorly.  

Each algorithm uses different learning rates and combinations of environment steps between updates. For example, we found that all AIL algorithms performed best when taking $10$ steps between updates, RL in the expert tended to work better by taking more steps in between updates ($\approx 400$) with a larger step-size $\approx 4\times 10^{-4}$, where the agents RL updates favored fewer steps $\approx 75$ with smaller steps $7 \times 10^{-5}$. For all algorithms $4$ parallel environments were run concurrently, as this was observed to improve performance across all algorithms. This was especially the case for the RL methods, which relied on more samples to accurately compute the advantage.  

We note that there is a point of diminishing returns for PPO specifically~\cite{Engstrom2020Implementation}, where policy learning degrades as the number of examples per update increases. Even though the advantage becomes progressively more accurate with increasing sample size, the mini-batch gradient decent procedure in PPO eventually leads to off-policy behavior that can be detrimental to learning.  We also found that pre-generating a number of trajectories and pretraining the value function tended to improve performance for both A2D, as well as the compact expert RL algorithm.  For A2D specifically, this ensured that the replay buffer for imitation learning was reasonably large prior to learning in the expert. This ensures that for any given update, the agent tends to be close to the expert policy, ensuring that the "off-policy" RL update is not too severely destabilized through importance weighting. To further improve this, we also introduced delayed policy updates, which further reduced the divergence between expert and the agent in A2D. In both A2D and the RL setups, this also helped ensure that the value function is always converging faster than the policy, ensuring that the error in the resulting advantage estimates are low.  

